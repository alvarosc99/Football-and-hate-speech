---
title: "1 - Train & Test"
author: '100385774'
date: "2023-04-27"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r warning=FALSE}
library(tidyverse)
library(data.table)
library(ggplot2)
library(stringr)
library(openxlsx)
library(lubridate)
library(emo)
library(stopwords)
library(tidytext)
library(caret)
library(textrecipes)
library(SnowballC)
library(rsample)
library(workflows)
library(discrim)
library(tidymodels)
```

```{r}

df <- read.xlsx("data/annotationbis.xlsx")
df <- df %>% mutate(
  hate = case_when(hate == 0 ~ "No", 
                   hate == 1 ~ "Yes"),
  hate = factor(hate, levels = c("No", "Yes"))
)

annotation_b <- read.delim("data/HaterNet.txt", sep = ";", col.names = c("id", "X1", "text", "X2", "hate")) %>% 
  select(-c(X1, X2))

annotation_c <- read.delim("data/hateval/hateval2019_es_train.csv", sep = ",") %>% 
  select(-c(TR, AG)) %>% 
  rename(hate = HS) 

```


Noise removal (bots), Boolean mention and URL presence, Boolean terms presence, lower casing, time conversion: 

```{r}
df <- df %>% 
  distinct(text, .keep_all = T) %>% 
  mutate(text = tolower(text),
         entities.mentions = ifelse(str_detect(text, "@.{1,15} "), 1, 0),
         entities.url = ifelse(str_detect(text, "https://.*\\b"), 1, 0),
         text = str_remove_all(text, "@.{1,15} "),
         text = str_remove_all(text, "https://.*\\b"),
         hateterm = str_detect(text, "(?:^|\\s)moros?\\b|(?:^|\\s)maric[oó]na?e?s?\\b|(?:^|\\s)monos?\\b|(?:^|\\s)putas?\\b|(?:^|\\s)hijos? de puta\\b|(?:^|\\s)hijos?deputa|(?:^|\\s)zorras?\\b|(?:^|\\s)panchitos?\\b"))

```

Emojis and laughs tokenization: 
https://regex101.com/r/UbZ90u/1

```{r}
df <- df %>% 
  mutate(text = str_replace_all(text, "🏻|🏼|🏽|🏾|🏿", ""),
         text = str_replace_all(text, "\\\\n|&lt", " "), 
         text = str_replace_all(text, "🤣+|😂+", " tokenrisa "),
         text = str_replace_all(text, "🐀+", " tokenrata "),
         text = str_replace_all(text, "😡+|🔪+|🤬+|👊+|🪓+|🤮+|💩+", " tokenenfado "),
         text = str_replace_all(text, "👏+|💪+|✊+|💪+|❤️+|🥰+|❤+|🤍+", " tokenpositivo "),
         text = ji_replace_all(text, ""),
         text = iconv(text, to = "UTF-8//IGNORE"),
         text = str_replace_all(text, "\\b(?:a*(?:ha*){2,}h?)\\b|\\b(?:a*(?:ja*)+j?)\\b|\\b(?:e*(?:je*)+j?)\\b|\\b(?:i*(?:ji+)+j?)\\b|\\b(?:A*(?:JA+)+J?)\\b|\\b(?:A*(?:HA+)+H?)\\b|\\b(?:e*(?:he+){2,}h?)\\b|\\b(?:Ja*(?:ja+)+j?)\\b|\\b(?:Je*(?:je+)+j?)\\b|\\bJa+\\b|\\b(?:Ji*(?:ji+)+j?)\\b|\\b(?:Ha*(?:ha+)+h?)\\b|\\b(?:Jo*(?:jo+)+j?)\\b|\\b(?:o*(?:jo+)+j?)\\b|\\b(?:a*((?:ja+)|(?:js+))+j?)\\b|\\b(?:A*((?:JA+)|(?:JS+))+J?)\\b|\\blo*l\\b", ":risa:"))

```

Other features factorize: 

```{r}
df <- df %>% 
  mutate(entities.url = factor(entities.url, levels = c(0,1)),
         entities.mentions = factor(entities.mentions, levels = c(0,1)),
         hateterm = case_when(hateterm == F ~ 0, 
                              hateterm == T ~ 1),
         hateterm = factor(hateterm, levels = c(0,1)))
```



Tokenizing and misspellings correction: 

```{r eval=FALSE, include=FALSE}
tidy_df <- df %>% 
  unnest_tokens(word, text)

tidy_df$word <- str_replace_all(tidy_df$word, "(?:^|\\s)q\\b", "que")
tidy_df$word <- str_replace_all(tidy_df$word, "(?:^|\\s)xq\\b", "porque")
tidy_df$word <- str_replace_all(tidy_df$word, "(?:^|\\s)xk\\b", "porque")
tidy_df$word <- str_replace_all(tidy_df$word, "(?:^|\\s)k\\b", "que")

```

Stopwords removal and stemming:

```{r eval=FALSE, include=FALSE}
#lemmas <- fread("Lemmatizer/200918/spdictionary.csv")
#lemmas <- lemmas[-which(word=="moro"),]

stop <- as.tibble(stopwords("es", source = "nltk")) %>% 
  rename(word = value) 
stop <- stop[!(stop$word %in% c("sin", "no", "contra", "poco", "pero")), ]

tidy_df <- tidy_df %>% 
  anti_join(stop) %>% 
  mutate(word = wordStem(word, language = "spanish"))

#tidy_df$word <- ifelse(is.na(match(tidy_df$word, lemmas$word)), tidy_df$word, lemmas$canonical[match(tidy_df$word, lemmas$word)])
  
```


## Building the model

Train and test partitions: 

```{r}
hate_split <- initial_split(df, strata = hate)

hate_train <- training(hate_split)
hate_test <- testing(hate_split)

dim(hate_train)
```

```{r}
hate_rec <- recipe(hate ~ text + entities.mentions + entities.url + hateterm, data = hate_train)
```

```{r}
hate_rec <- hate_rec %>% 
  step_tokenize(text) %>% 
  step_stopwords(text, language = "es", keep = F) %>% 
  step_stem(text, options = list(language = "es")) %>% 
  step_tfidf(text)

hate_wf <- workflow() %>% 
  add_recipe(hate_rec)
```

```{r}
nb_spec <- naive_Bayes() %>% 
  set_mode("classification") %>% 
  set_engine("naivebayes")
```

```{r}
nb_fit <- hate_wf %>% 
  add_model(nb_spec) %>% 
  fit(data = hate_train)
```

## Cross-validation

```{r}
set.seed(123)
hate_folds <- vfold_cv(hate_train)
```

```{r}
hate_wf <- workflow() %>%
add_recipe(hate_rec) %>%
add_model(nb_spec)
```

```{r}
nb_rs <- fit_resamples(
hate_wf,
hate_folds,
control = control_resamples(save_pred = TRUE)
)
```

```{r}
nb_rs_metrics <- collect_metrics(nb_rs)
nb_rs_predictions <- collect_predictions(nb_rs)

nb_rs_metrics
nb_rs_predictions
```

```{r}
conf_mat_resampled(nb_rs, tidy = FALSE) %>%
autoplot(type = "heatmap")
```

# Logistic regression

```{r}
hate_wf2 <- workflow() %>% 
  add_recipe(hate_rec)
```

```{r}
glm_spec <- discrim_linear() %>% 
  set_mode("classification") %>% 
  set_engine("MASS")
```

```{r}
glm_fit <- hate_wf2 %>% 
  add_model(glm_spec) %>% 
  fit(data = hate_train)
```

## Cross-validation

```{r}
set.seed(123)

hate_wf2 <- workflow() %>%
add_recipe(hate_rec) %>%
add_model(glm_spec)
```

```{r}
glm_rs <- fit_resamples(
hate_wf2,
hate_folds,
control = control_resamples(save_pred = TRUE)
)
```

```{r}
glm_rs_metrics <- collect_metrics(glm_rs)
glm_rs_predictions <- collect_predictions(glm_rs)

glm_rs_metrics
```

```{r}
conf_mat_resampled(nb_rs, tidy = FALSE) %>%
autoplot(type = "heatmap")
```

# Caret

```{r}
dtm <- tidy_df %>% 
  count(id, word, sort = T) %>% 
  bind_tf_idf(word, id, n) %>% 
  cast_dtm(id, word, tf_idf)

dtm
```

```{r}
require(tm)
dtm <- removeSparseTerms(dtm, sparse = .99)
dtm
```

```{r}
meta <- tibble(id = as.numeric(dimnames(dtm)[[1]])) %>%
  left_join(df[!duplicated(df$id), ], by = "id")
```

```{r}
set.seed(1234)
trainIndex <- createDataPartition(meta$hate, p = 0.8, list = FALSE, times = 1)
```

```{r}
df_train <- dtm[trainIndex, ] %>% as.matrix() %>% as.data.frame()
df_test <- dtm[-trainIndex, ] %>% as.matrix() %>% as.data.frame()

response_train <- df$hate[trainIndex]
```

## rf

```{r}
ctrl <- trainControl(method = "none")
```


```{r}
rf_mod <- train(x = df_train, 
                y = as.factor(response_train), 
                method = "ranger",
                trControl = ctrl,
                tuneGrid = data.frame(mtry = floor(sqrt(dim(df_train)[2])),
                                      splitrule = "gini",
                                      min.node.size = 1))
```


```{r}
rf_pred <- predict(rf_mod,
                   newdata = df_test)



rf_cm <- confusionMatrix(rf_pred, meta[-trainIndex, ]$hate)
rf_cm
```





